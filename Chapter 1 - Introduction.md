# Chapter 1 Introduction

I was a child when I first watched the movie “Inception” and was amazed by Christopher Nolan's imagination about “deep”. Build a layer of dreams enables the dream stealers to peek into the privacy of others and step into their inner world. What about two, three, or deeper dreams? In the movie, Leonardo DiCaprio builds a three-layered dream to help Japanese businessman Saito implant ideas in the head of the rival’s son. Such a fantasy story will not happen in real life, but today, with the rapid development of artificial intelligence, people's lives have undergone tremendous changes. Due to the continuous development of the neural network and the rise of deep learning, as researchers, can we also use the method of deep learning to "implant" a better life into the future of humankind? This dissertation might not directly give the answers, but we will be able to generate some ideas of our own afterward.

### 1.1  Background
Deep learning is occupying the world. About a decade ago, deep learning was just an academic term stuck in the lab. Today, however, it becomes part of people's lives. The term is no longer just for researchers, but for people to talk about after meals and suddenly become the “future” in the eyes of investors. So, what is deep learning? Why did it attract so much attention? What can people do with it? In this section, we will introduce the most fundamental building blocks of deep learning from scratch, with a glance at some advanced principles and applications.

#### 1.1.1  Neural Networks
Neural networks were born a few decades ago. At that time, researchers had not learned about its extraordinary performance, and more of it just stayed in the corner of the laboratory as an upset direction. The neural network at that time was also very different from the current mainstream neural network. The former has only a simple network topology, few layers, and low-efficiency optimization, while the latter has a rich structure and derivatives, various optimization techniques, and much more efficient computing capabilities. Neural networks have gradually matured with ups and downs and become a hot subject today.

There are many versions of the definition of neural networks. Some people think of it as a simulation of the structure and functioning of the human brain and link it to biologies such as neurology and brain science. In my opinion, a neural network is a bridge (mathematical tool) capable of linking (mapping) low-dimensional input space to high-dimensional feature space, so that problems become separable. Neurons are the fundamental building blocks for a neural network. In general, three operations take place in a neuron, i.e., the matrix multiplication, broadcasting, and nonlinear mapping (activation). Take logistic regression as an example. For a training example that is represented as a feature vector, the neuron first multiplies the vector with its weight (matrix multiplication), then adds a bias (broadcasting), and finally maps the results with a softmax unit (nonlinear mapping). 

Following a similar pattern and stacking a lot of logistic regression units together leads to a feed-forward neural network architecture (see Figure 1.1). For this kind of network structure, the input features are fed in at the input layer, then processed by one or more layers with weights, biases, and activations in a neuron-by-neuron, layer-by-layer manner, and then outputted at the output layer as output signals. This is called the forward propagation, and the weights and biases remain unchanged during this stage. Starting from the output layer, the output signals are used to calculate the error signals by some loss criteria. The error signals are then passed backward through the network, layer-by-layer, and recursively compute the local gradients and update the weights and biases accordingly. This consists of backward propagation. Some scientific research reports on the performance of these simple-structured neural networks and applied it to classification tasks such as handwritten digit recognition. We will discuss the details of the forward and backward propagations in Chapter 2. 

Some pioneers in the field of machine learning have shown that neural networks do show advantages in solving problems such as regression and classification. Experiments have illustrated some results where neural networks outperform support vector machine (SVM) in some tasks, while SVM has dominated the area of data-driven processing for many years. However, why did the neural network become popular decades after its birth? Computational efficiency is an important reason. The performance of the neural network becomes better as the number of layers increases, but as the structure becomes complex, the amount of calculations also increases. Computers decades ago did not have as much computing power as today, so neural network models at that time were relatively shallow and straightforward. Even if some researchers try to construct complex network models, they are limited in computing power, and even if the results are positive, they cannot be applied to real life. Another important reason is the calculation of local graidents in neural networks. As an important part of weight update, the proposal of backward propagation is of great significance to neural networks. With the increase of layers, how to calculate the gradient of neron becomes an indicator to determine whether a neural network model is effective. Based on the above reasons, neural networks attracted attention when they were born, but have been silent for a long time after that.

#### 1.1.2  Deep Learning
To some extent, deep learning is equivalent to the very deep neural network. The trend of deep learning is that with the increasing of the number of layers, the dimensions of the feature spaces can be higher and higher, and this could result in better prediction performance. As the growth of computational power, a neural network with hundreds and even thousands of layers becomes possible. One of the advantages of deep learning is that we do not need to know what explicitly the decision boundary is, while the model can generate the classifier itself. All we need to do is to define the configurations of the neural network, for example, the number of layers in the network, the number of neurons in a layer, use what kind of activation functions, whether using optimization techniques for backward propagation or not, etc.

Neural networks have become deeper and deeper, with the state-of-the-art network models going from just a few layers (e.g., LeNet [1]) and AlexNet [2]) to over a hundred layers (e.g., Inception [3]). However, some problems occur with the number of layers increasing, such as the exploding and vanishing gradient problems. In general, the benefits and problems of very deep neural networks can be concluded as follows:

+	The main benefits of a very deep neural network are that it has the ability to represent very complex functions and it can learn features at different levels of abstraction, from edges (at the shallower layers, closer to the input) to sophisticated features (at the deeper layers, closer to the output).

### 1.2  Motivation
.
